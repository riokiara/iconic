---
title: "Iconic Challenge - Stage 3"
output: 
  github_document:
    html_preview: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Install Packages
list.of.packages <- c("data.table"
                      ,"httr"
                      ,"jsonlite"
                      ,"digest"
                      ,"Hmisc"
                      ,"RJSONIO"
                      ,"RSQLite"
                      ,"ggplot2"
                      ,"caret"
                      ,"Rtsne"
                      ,"ggcorrplot"
                      ,"keras"
                      )
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, dependencies = T)

# Load Packages
library(data.table)
library(jsonlite)
library(httr)
library(curl)
library(digest)
library(Hmisc)
library(RJSONIO)
library(RSQLite)
library(ggplot2)
library(caret)
library(Rtsne)
library(ggcorrplot)
library(keras)
install_keras()
```

## Step 1: Load the cleaned dataset

Load from the local directory
```{r}
customerDT <- fread("../Data/cleanedData.csv")
```

## Step 2: Clustering Analysis

Since we do not have labelled data, we would perform an unsupervised learning    
We are aware that customers shop for either female or male or unisex items, so would be looking for at least 3 clusters that splits customers based on their gender preference.
But let's try to find optimal number of clusters first   

```{r message=FALSE, warning=FALSE}
# Initialize total within sum of squares error: wss
wss <- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out <- kmeans(scale(customerDT[,2:43]), centers = i,nstart = 2, iter.max = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")


```

Looking at the plot, there is no clear bend in the plot that suggests optimal clusters  
Let's attempt to run clustering using 3 clusters and look at the accuracy  

```{r message=FALSE, warning=FALSE}

# Clustering using all the dimensions and 3 clusters
clusters <- kmeans(scale(customerDT[,2:43]), 3, nstart = 2)
clusters
```

Between/Within accuracy percentage: (between_SS / total_SS) =  22.1 %  
The accuracy is low and the cluster sizes does not correspond to the customer's shopping behaviour in terms of gender specific purchases.

```{r}
# Customers who prefer female merchandise over male/unisex merchandise
customerDT[female_items > male_items + unisex_items, .N/nrow(customerDT)*100]
```

I am expecting one of cluster size to be around 67% of the obervations.
So, we can't really infer the clusters to be gender using this method of unsupervised learning  

Let's try looking at reducing dimensionality using t-SNE algorithm

```{r message=FALSE, warning=FALSE}
## Rtsne function may take some minutes to complete...
set.seed(131)  
tsne_model_1 = Rtsne(as.matrix(customerDT[,2:43]), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2)

## getting the two dimension matrix
d_tsne_1 = as.data.frame(tsne_model_1$Y) 

setDT(d_tsne_1)

ggplot(d_tsne_1, aes(x=V1, y=V2)) +  
  geom_point(size=0.25) +
  guides(colour=guide_legend(override.aes=list(size=6))) +
  xlab("") + ylab("") +
  ggtitle("t-SNE") +
  theme_light(base_size=20) +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank()) +
  scale_colour_brewer(palette = "Set2")
```

Now let's try clustering again using the reduced dimensions

```{r message=FALSE, warning=FALSE}
## keeping original data
d_tsne_1_original=d_tsne_1

# Initialize total within sum of squares error: wss
wss <- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out <- kmeans(scale(d_tsne_1), centers = i,nstart = 2, iter.max = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")
```

Again, looking at the plot, there is no clear bend in the plot that suggests optimal clusters  
Let's try fitting with 3 clusters  

```{r}
## Creating k-means clustering model, and assigning the result to the data used to create the tsne
fit_cluster_kmeans=kmeans(scale(d_tsne_1), 3)  
fit_cluster_kmeans
d_tsne_1_original$cl_kmeans = factor(fit_cluster_kmeans$cluster)

ggplot(d_tsne_1_original, aes_string(x="V1", y="V2", color="cl_kmeans")) +
  geom_point(size=0.25) +
  guides(colour=guide_legend(override.aes=list(size=6))) +
  xlab("") + ylab("") +
  ggtitle("") +
  theme_light(base_size=20) +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        legend.direction = "horizontal", 
        legend.position = "bottom",
        legend.box = "horizontal") + 
  scale_colour_brewer(palette = "Accent") 
```

Again, we can't infer the clusters to correspond to customers' preferred gender merchandise  

## Step 3: Building a classification model

Next we can try to do is to fit a supervised learning algorithm. Before that, we would need labels of inferred gender for each customer.  

We will use customers' preferred merchandise purchase behaviour to infer the gender  

```{r}
# Customers who buy more female merchandise are inferred as females
customerDT[female_items > male_items + unisex_items, inf_gender := 1]
# Customers who buy more male merchandise are inferred as males
customerDT[male_items > female_items + unisex_items, inf_gender := 0]
# Customers who buy more female/unisex merchandise are inferred as females
customerDT[is.na(inf_gender) & male_items == 0 & female_items >= unisex_items, inf_gender:= 1]
# Customers who buy more male/unisex merchandise are inferred as males
customerDT[is.na(inf_gender) & female_items == 0 & male_items >= unisex_items, inf_gender:= 0]
customerDT[is.na(inf_gender) & (female_items > male_items), inf_gender:= 1]
customerDT[is.na(inf_gender) & (male_items > female_items), inf_gender:= 0]

# For remaining customer, we assume that customer who used discounts or pay later options are females, otherwise we assume them as males
customerDT[is.na(inf_gender) & (average_discount_used > 0 | afterpay_payments >0), inf_gender:= 1]
customerDT[is.na(inf_gender) & !(average_discount_used > 0 | afterpay_payments >0), inf_gender:= 0]
```

Split the data in to train and test sets  

```{r}
# Set the seed to get reproducible results
set.seed(131)

# Determine sample size
ind <- sample(2, nrow(customerDT), replace=TRUE, prob=c(0.67, 0.33))

# Split the data
customerDT.train <- customerDT[ind==1, 2:43]
customerDT.test <- customerDT[ind==2, 2:43]
customerDT.train <- normalize(as.matrix(customerDT.train))
customerDT.test <- normalize(as.matrix(customerDT.test))

# Split the class attribute
customerDT.traintarget <- as.matrix(customerDT[ind==1, 44])
customerDT.testtarget <- as.matrix(customerDT[ind==2, 44])
```

We will use a deep learning model for this supervised learning problem  
First step is to initialise a sequential model and add layers  

```{r}
# Initialize a sequential model
model <- keras_model_sequential()

# Add layers to the model
model %>% 
  layer_dense(units = 20, activation = 'relu', input_shape = c(42)) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 20, activation = 'relu') %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 10, activation = 'relu') %>% 
  layer_dropout(rate = 0.3) %>% 
  layer_dense(units = 1, activation = 'sigmoid')
```

We will use an optimiser to tune our model  
```{r}
# Define an optimizer (lr is the learning rate that can be tuned)
rmsprop <- optimizer_rmsprop(lr = 0.001)
```

Compiling and fitting the model 
```{r}
# Compile the model
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = rmsprop,
  metrics= 'accuracy'
)


# Fit the model 
history <- model %>% fit(
  customerDT.train, 
  customerDT.traintarget, 
  epochs = 200, 
  batch_size = 128, 
  validation_split = 0.2
)
```

Plotting the validation loss and accuracy on the train set  
```{r}
plot(history)
```

Confusion matrix  
```{r}
# Predict the classes for the test data
classes <- model %>% predict_classes(customerDT.test)
# Confusion matrix
table(customerDT.testtarget, classes)
```

Model validation on the test set  
```{r}
model %>% evaluate(customerDT.test, customerDT.testtarget)
```

Looking at the confusion matrix, there are high number of incorrect predictions for male gender  
The accuracy is really good on the test set  
The model can be tuned further by using different layers and/or using different optimiser  
Also, the hyper parameters can be tuned to get better results  
But the model will be only as good as our data (derived gender labels)   