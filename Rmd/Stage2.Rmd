---
title: "Iconic Challenge - Stage 2"
output: 
  github_document:
    html_preview: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Install Packages
list.of.packages <- c("data.table"
                      ,"httr"
                      ,"jsonlite"
                      ,"digest"
                      ,"Hmisc"
                      ,"RJSONIO"
                      ,"RSQLite"
                      )
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, dependencies = T)

# Load Packages
library(data.table)
library(jsonlite)
library(httr)
library(curl)
library(digest)
library(Hmisc)
library(RJSONIO)
library(RSQLite)
```

## Step 1: Download the password protected compressed json file

Download from the git repository  
```{r}
filep <- "https://github.com/theiconic/datascientist/raw/master/test_data.zip"
filename <- "../Data/test_data.zip"
download.file(url=filep, destfile=filename)
```

## Step 2: Hashed Password

The password can be managed in a better way. Example: store it in a config file or credential store  

```{r}
filepwd <- digest(object = 'welcometotheiconic', algo = 'sha256', serialize = FALSE)
```

## Step 3:  Extact zip file.
Note: This requires an installation of 7zip (Third Party Software)  

```{r}
sys_command <- paste0("7z ", "x ", filename, " -p", filepwd, " -o../Data -aoa")
system(sys_command, intern = F)
```

## Step 4:  Load JSON file.

Load the extracted JSON file  

```{r}
jsonfile <- "../Data/data.json"
fjson <- read_json(jsonfile)
```

## Step 5:  Convert JSON to R data.table

```{r}
customerDT <- rbindlist(fjson, fill = T)

#Summarize data table
summary(customerDT)
```

## Step 6:  Finding anomalies in the data
As mentioned in the instructions, the data looks correct from the summary. 
There are 10,205 nulls in coupon_discount_applied. Other columns do not have any nulls  
At this stage, we just assume the nulls to be 0 but these can be imputed with the help of other information on how the discounts are used by customer. For that, we would need more business knowledge around the definition and usage of coupon_discount_applied  

```{r}
# Impute nulls with 0
customerDT[is.na(coupon_discount_applied) == T, coupon_discount_applied:=0]
```

Convert is_newsletter_subscriber to numeric. This will help in further analysis where some algorithms expect data to be numeric  

```{r message=FALSE, warning=FALSE}
customerDT[is_newsletter_subscriber == 'N',is_newsletter_subscriber := 0]
customerDT[is_newsletter_subscriber == 'Y',is_newsletter_subscriber := 1]
customerDT[, is_newsletter_subscriber := as.numeric(is_newsletter_subscriber)]
```

Find duplicates in the data  

```{r}
#Unique customers
customerDT[,uniqueN(customer_id)]
```

There are 46,279 Customers in the dataset out of which 46,030 are unique.  
Let us look at some of the duplicate customer data  

```{r}
customerDT[customer_id %in%head(customerDT[duplicated(customer_id) == T,customer_id],5)][order(customer_id)]
```

Looking at the customer details, it is confirmed that these customers have entire details duplicated.  
Let's remove the records which are completely duplicated as they do not add to the information already available.  

```{r}
customerDT <- unique(customerDT)
#Check again for duplicate customer ids
customerDT[duplicated(customer_id) == T,customer_id]
```

So, all the duplicate customer ids had their entire details duplicated.  
It seems these customers were intentionally duplicated and hence they have been removed from analysis.  

```{r}
#Check days since last order and first order
customerDT[days_since_last_order > days_since_first_order, .N*100/nrow(customerDT)]
```

More than 94% of the records have, days since last order greater than days since first order.  
It appears that these are the two columns where although the data looks correct but isn't really correct.  
These two columns appear to be swapped  

```{r}
#Fix issue with days since last order and first order
customerDT[days_since_last_order > days_since_first_order, c('days_since_last_order', 'days_since_first_order'):=.(days_since_first_order,days_since_last_order)]
```

The range of days is in years, so this data about the customer seems to be shifted by some deviation.
Also, comparing the average_discount_used with the revenue figures, it seems either of them or both of them have been shifted by some deviation.


```{r}
# More cancellations than orders per customer
customerDT[cancels > orders, .N*100/nrow(customerDT)]

# More returns than orders per customer
customerDT[returns > orders, .N*100/nrow(customerDT)]

# No revenue generating customers
customerDT[(revenue + redpen_discount_used + coupon_discount_applied + average_discount_used) <= 0 & (orders-cancels-returns > 0), .N*100/nrow(customerDT)]
```
There are very few instances where a customer has cancelled more than they ordered(.01%)  
There are around 4% customers who have returned more orders than they have placed  
And, there are around 1% customers who haven't generated a revenue after considering all cancellations, returns and discounts  
We are not sure why we have these anomalies. It could be intentional data manipulation or caused because of applying deviations to the original data.  
Not treating these anomalies at the moment.

```{r, dpi=200}
# Outlier Analysis on customer orders
qplot(y=customerDT$orders, x= 1, geom = "boxplot")+
  labs(x = "Customers", y = "Number of Orders") +
  labs(title="Box Plot of Customer Orders", size = 15) +
  theme(panel.background = element_blank(), axis.line = element_line(colour = "grey"),  plot.title = element_text(hjust = 0.5))

# Extracting the orders in the upper whisker of the box plot
customerDT[orders>max(boxplot.stats(customerDT$orders)$stats), list(orders)][order(-orders)]
```
The same analysis can be done on all the details of the cutomers.  
We may find some outliers but at this stage, without more information about the exact cause of the extreme outlier values, we are leaving them as-is in the dataset.  

## Step 7:  Save Cleaned Dataset to a file
```{r}
# Save cleaned dataset to a file
fwrite(customerDT, "../Data/cleanedData.csv")
```

[Stage 3](Stage3.md)  
